version: "3"

services:       #Services à exécuter
  apache_img_1:     #Premier serveur apache à démarrer / Service 1
    container_name: ${COMPOSE_PROJECT_NAME}_con_apache_1
    build: ./apache/1     #tag de l'image construite
    restart: always
    expose:               #Ports de notre service (port interne)
      - ${APACHE_EXPOSED_PORT}
    networks:
      public_net:           #Custom name
        ipv4_address: ${APACHE_1_IP}
  apache_img_2:       #Deuxième serveur apache à démarrer / Service 2
    container_name: ${COMPOSE_PROJECT_NAME}_con_apache_2
    build: ./apache/2
    restart: always
    expose:
      - ${APACHE_EXPOSED_PORT}
    networks:
      public_net:
        ipv4_address: ${APACHE_2_IP}
  haproxy:            #Service haproxy : load balancing + proxy pour TCP et applications HTTP
    build: ./haproxy  
    container_name: ${COMPOSE_PROJECT_NAME}_haproxy_1
    ports:
      - 80:80
    expose:
      - 80
    networks:
      public_net:
        ipv4_address: ${HA_PROXY_IP}
    environment:      #Passage de notre shell actuel aux services des conteneurs ci-dessous
      - APACHE_1_IP=${APACHE_1_IP}
      - APACHE_2_IP=${APACHE_2_IP}
      - APACHE_EXPOSED_PORT=${APACHE_EXPOSED_PORT}
    restart: always

  # Chaos engineering, killing each 30s a container
  pumba:
    image: gaiaadm/pumba
    container_name: chaos_with_pumba
    restart: always
    #the --signal=SIGKILL would be definitely more effective, but no restart mechanic yet except the restart:always
    command: --interval=30s --random --log-level=info kill --signal=SIGTERM "re2:^${COMPOSE_PROJECT_NAME}" 
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    
  esmaster:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.0
    container_name: ${COMPOSE_PROJECT_NAME}_esmaster_1
    restart: always
    environment:
      - node.name=esmaster
      - node.master=true
      - node.voting_only=false 
      - node.data=false 
      - node.ingest=false 
      - node.ml=false 
      - xpack.ml.enabled=true 
      - cluster.remote.connect=false
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=esdata1
      - cluster.initial_master_nodes=esmaster,esdata1
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - datamaster:/usr/share/elasticsearch/data
    ports:
      - 127.0.0.1:9200:9200
    networks:
      - elastic
      
  esdata1:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.0
    container_name: ${COMPOSE_PROJECT_NAME}_esdata_1
    restart: always
    environment:
      - node.name=esdata1
      - node.master=false
      - node.voting_only=false 
      - node.data=true 
      - node.ingest=false 
      - node.ml=false 
      - cluster.remote.connect=false 
      #would be great to make it an ingest node for pipeline process
      #by default ingest is set to true for all nodes
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=esmaster
      - cluster.initial_master_nodes=esmaster,esdata1
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data1:/usr/share/elasticsearch/data
    networks:
      - elastic
      
  kibana:     #Service utilisé pour kibana
    image: docker.elastic.co/kibana/kibana:6.3.2
    container_name: ${COMPOSE_PROJECT_NAME}_kibana_1
    ports:
      - "5601:5601"
    restart: always

networks:
  public_net:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET}
  elastic:
    driver: bridge

volumes:
  datamaster:
    driver: local
  data1:
    driver: local
